---
id: policies
sidebar_label: Policies
title: Policies
description: Define and train customized policy configurations to optimize your contextual assistant for longer contexts or unseen utterances which require generalization.
---

<!-- TODO: replace with 2 files for ML policies and RulePolicy -->

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="policy-file"></a>

## Configuring Policies

The `rasa.core.policies.Policy` class decides which action to take
at every step in the conversation.

There are different policies to choose from, and you can include
multiple policies in a single configuration.

:::note
Per default a maximum of 10 next actions can be predicted
by the agent after every user message. To update this value
you can set the environment variable `MAX_NUMBER_OF_PREDICTIONS`
to the desired number of maximum predictions.

:::

Your project's `config.yml` file takes a `policies` key
which you can use to customize the policies your assistant uses.
You can also leave this key out or empty. Then the
[Suggested Config](.//model-configuration.mdx#suggested-config)
feature will choose some default policies for you.

You can also reference custom policies in your configuration. In the example below, the
last two lines show how to use a custom policy class and pass arguments to it.

```yaml-rasa
policies:
  - name: "TEDPolicy"
    featurizer:
    - name: MaxHistoryTrackerFeaturizer
      max_history: 5
      state_featurizer:
        - name: BinarySingleStateFeaturizer
  - name: "RulePolicy"
  - name: "path.to.your.policy.class"
    arg1: "..."
```

### Max History

One important hyperparameter for Rasa Core policies is the `max_history`.
This controls how much dialogue history the model looks at to decide which
action to take next.

You can set the `max_history` by passing it to your policy's `Featurizer`
in the policy configuration yaml file.

:::note
Only the `MaxHistoryTrackerFeaturizer` uses a max history,
whereas the `FullDialogueTrackerFeaturizer` always looks at
the full conversation history. See [Featurization of Conversations](./policies.mdx#featurization-conversations) for details.

:::

As an example, let's say you have an `out_of_scope` intent which
describes off-topic user messages. If your bot sees this intent multiple
times in a row, you might want to tell the user what you can help them
with. So your story might look like this:

```yaml-rasa
stories:
  - story: utter help after 2 fallbacks
    steps:
    - intent: out_of_scope
    - action: utter_default
    - intent: out_of_scope
    - action: utter_default
    - intent: out_of_scope
    - action: utter_help_message
```

For Rasa Core to learn this pattern, the `max_history`
has to be at least 4.

If you increase your `max_history`, your model will become bigger and
training will take longer. If you have some information that should
affect the dialogue very far into the future, you should store it as a
slot. Slot information is always available for every featurizer.

### Data Augmentation

When you train a model, by default Rasa Core will create
longer stories by randomly gluing together
the ones in your stories files.
This is because if you have stories like:

```yaml-rasa
stories: 
  - story: thank
    steps:
    - intent: thankyou
    - action: utter_youarewelcome
  - story: say goodbye
    steps: 
    - intent: goodbye
    - action: utter_goodbye
```

You actually want to teach your policy to **ignore** the dialogue history
when it isn't relevant and just respond with the same action no matter
what happened before.

You can alter this behavior with the `--augmentation` flag.
Which allows you to set the `augmentation_factor`.
The `augmentation_factor` determines how many augmented stories are
subsampled during training. The augmented stories are subsampled before training
since their number can quickly become very large, and we want to limit it.
The number of sampled stories is `augmentation_factor` x10.
By default augmentation is set to 20, resulting in a maximum of 200 augmented stories.

`--augmentation 0` disables all augmentation behavior.
The memoization based policies are not affected by augmentation
(independent of the `augmentation_factor`) and will automatically
ignore all augmented stories.

## Action Selection

At every turn, each policy defined in your configuration will
predict a next action with a certain confidence level. For more information
about how each policy makes its decision, read into the policy's description below.
The bot's next action is then decided by the policy that predicts with the highest confidence.

In the case that two policies predict with equal confidence (for example, the Memoization
and Mapping Policies always predict with confidence of either 0 or 1), the priority of the
policies is considered. Rasa policies have default priorities that are set to ensure the
expected outcome in the case of a tie. They look like this, where higher numbers have higher priority:

1. `TEDPolicy` and `SklearnPolicy`

2. `MappingPolicy`

3. `MemoizationPolicy` and `AugmentedMemoizationPolicy`

4. `FallbackPolicy` and `TwoStageFallbackPolicy`

5. `RulePolicy` and `FormPolicy`

This priority hierarchy ensures that, for example, if there is an intent with a mapped action, but the NLU confidence is not
above the `nlu_threshold`, the bot will still fall back. In general, it is not recommended to have more
than one policy per priority level, and some policies on the same priority level, such as the two
fallback policies, strictly cannot be used in tandem.

If you create your own policy, use these priorities as a guide for figuring out the priority of your policy.
If your policy is a machine learning policy, it should most likely have priority 1, the same as the Rasa machine
learning policies.

:::caution
All policy priorities are configurable via the `priority:` parameter in the configuration,
but we **do not recommend** changing them outside of specific cases such as custom policies.
Doing so can lead to unexpected and undesired bot behavior.

:::

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="embedding-policy"></a>

## Embedding Policy

:::caution
`EmbeddingPolicy` was renamed to `TEDPolicy`. Please use [TED Policy](./policies.mdx#ted-policy) instead of `EmbeddingPolicy`
in your policy configuration. The functionality of the policy stayed the same.

:::

## TED Policy

The Transformer Embedding Dialogue (TED) Policy is described in
[our paper](https://arxiv.org/abs/1910.00486).

This policy has a pre-defined architecture, which comprises the
following steps:

* concatenate user input (user intent and entities), previous system actions, slots and active forms for each time
  step into an input vector to pre-transformer embedding layer;

* feed it to transformer;

* apply a dense layer to the output of the transformer to get embeddings of a dialogue for each time step;

* apply a dense layer to create embeddings for system actions for each time step;

* calculate the similarity between the dialogue embedding and embedded system actions.
  This step is based on the [StarSpace](https://arxiv.org/abs/1709.03856) idea.

It is recommended to use `state_featurizer=LabelTokenizerSingleStateFeaturizer(...)`
(see [Featurization of Conversations](./policies.mdx#featurization-conversations) for details).

**Configuration:**

Configuration parameters can be passed as parameters to the `TEDPolicy` within the configuration file.
If you want to adapt your model, start by modifying the following parameters:

* `epochs`:
  This parameter sets the number of times the algorithm will see the training data (default: `1`).
  One `epoch` is equals to one forward pass and one backward pass of all the training examples.
  Sometimes the model needs more epochs to properly learn.
  Sometimes more epochs don't influence the performance.
  The lower the number of epochs the faster the model is trained.

* `hidden_layers_sizes`:
  This parameter allows you to define the number of feed forward layers and their output
  dimensions for dialogues and intents (default: `dialogue: [], label: []`).
  Every entry in the list corresponds to a feed forward layer.
  For example, if you set `dialogue: [256, 128]`, we will add two feed forward layers in front of
  the transformer. The vectors of the input tokens (coming from the dialogue) will be passed on to those
  layers. The first layer will have an output dimension of 256 and the second layer will have an output
  dimension of 128. If an empty list is used (default behavior), no feed forward layer will be
  added.
  Make sure to use only positive integer values. Usually, numbers of power of two are used.
  Also, it is usual practice to have decreasing values in the list: next value is smaller or equal to the
  value before.

* `number_of_transformer_layers`:
  This parameter sets the number of transformer layers to use (default: `1`).
  The number of transformer layers corresponds to the transformer blocks to use for the model.

* `transformer_size`:
  This parameter sets the number of units in the transformer (default: `128`).
  The vectors coming out of the transformers will have the given `transformer_size`.

* `weight_sparsity`:
  This parameter defines the fraction of kernel weights that are set to 0 for all feed forward layers
  in the model (default: `0.8`). The value should be between 0 and 1. If you set `weight_sparsity`
  to 0, no kernel weights will be set to 0, the layer acts as a standard feed forward layer. You should not
  set `weight_sparsity` to 1 as this would result in all kernel weights being 0, i.e. the model is not able
  to learn.

:::caution
Pass an appropriate number, for example 50,  of `epochs` to the `TEDPolicy`, otherwise the policy will
be trained only for `1` epoch.

:::

:::caution
Default `max_history` for this policy is `None` which means it'll use the
`FullDialogueTrackerFeaturizer`. We recommend to set `max_history` to some finite value in order to
use `MaxHistoryTrackerFeaturizer` for **faster training**. See [Featurization of Conversations](./policies.mdx#featurization-conversations) for
details. We recommend to increase `batch_size` for `MaxHistoryTrackerFeaturizer`
(e.g. `"batch_size": [32, 64]`)

:::

<details><summary>The above configuration parameters are the ones you should configure to fit your model to your data.
However, additional parameters exist that can be adapted.</summary>

```
+---------------------------------+------------------+--------------------------------------------------------------+
| Parameter                       | Default Value    | Description                                                  |
+=================================+==================+==============================================================+
| hidden_layers_sizes             | dialogue: []     | Hidden layer sizes for layers before the embedding layers    |
|                                 | label: []        | for dialogue and labels. The number of hidden layers is      |
|                                 |                  | equal to the length of the corresponding.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| transformer_size                | 128              | Number of units in transformer.                              |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_transformer_layers    | 1                | Number of transformer layers.                                |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_attention_heads       | 4                | Number of attention heads in transformer.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_key_relative_attention      | False            | If 'True' use key relative embeddings in attention.          |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_value_relative_attention    | False            | If 'True' use value relative embeddings in attention.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| max_relative_position           | None             | Maximum position for relative embeddings.                    |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_size                      | [8, 32]          | Initial and final value for batch sizes.                     |
|                                 |                  | Batch size will be linearly increased for each epoch.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| batch_strategy                  | "balanced"       | Strategy used when creating batches.                         |
|                                 |                  | Can be either 'sequence' or 'balanced'.                      |
+---------------------------------+------------------+--------------------------------------------------------------+
| epochs                          | 1                | Number of epochs to train.                                   |
+---------------------------------+------------------+--------------------------------------------------------------+
| random_seed                     | None             | Set random seed to any 'int' to get reproducible results.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| embedding_dimension             | 20               | Dimension size of embedding vectors.                         |
+---------------------------------+------------------+--------------------------------------------------------------+
| number_of_negative_examples     | 20               | The number of incorrect labels. The algorithm will minimize  |
|                                 |                  | their similarity to the user input during training.          |
+---------------------------------+------------------+--------------------------------------------------------------+
| similarity_type                 | "auto"           | Type of similarity measure to use, either 'auto' or 'cosine' |
|                                 |                  | or 'inner'.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| loss_type                       | "softmax"        | The type of the loss function, either 'softmax' or 'margin'. |
+---------------------------------+------------------+--------------------------------------------------------------+
| ranking_length                  | 10               | Number of top actions to normalize scores for loss type      |
|                                 |                  | 'softmax'. Set to 0 to turn off normalization.               |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_positive_similarity     | 0.8              | Indicates how similar the algorithm should try to make       |
|                                 |                  | embedding vectors for correct labels.                        |
|                                 |                  | Should be 0.0 < ... < 1.0 for 'cosine' similarity type.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| maximum_negative_similarity     | -0.2             | Maximum negative similarity for incorrect labels.            |
|                                 |                  | Should be -1.0 < ... < 1.0 for 'cosine' similarity type.     |
+---------------------------------+------------------+--------------------------------------------------------------+
| use_maximum_negative_similarity | True             | If 'True' the algorithm only minimizes maximum similarity    |
|                                 |                  | over incorrect intent labels, used only if 'loss_type' is    |
|                                 |                  | set to 'margin'.                                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| scale_loss                      | True             | Scale loss inverse proportionally to confidence of correct   |
|                                 |                  | prediction.                                                  |
+---------------------------------+------------------+--------------------------------------------------------------+
| regularization_constant         | 0.001            | The scale of regularization.                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
| negative_margin_scale           | 0.8              | The scale of how important it is to minimize the maximum     |
|                                 |                  | similarity between embeddings of different labels.           |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate_dialogue              | 0.1              | Dropout rate for embedding layers of dialogue features.      |
|                                 |                  | Value should be between 0 and 1.                             |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate_label                 | 0.0              | Dropout rate for embedding layers of label features.         |
|                                 |                  | Value should be between 0 and 1.                             |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| drop_rate_attention             | 0.0              | Dropout rate for attention. Value should be between 0 and 1. |
|                                 |                  | The higher the value the higher the regularization effect.   |
+---------------------------------+------------------+--------------------------------------------------------------+
| weight_sparsity                 | 0.8              | Sparsity of the weights in dense layers.                     |
|                                 |                  | Value should be between 0 and 1.                             |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_every_number_of_epochs | 20               | How often to calculate validation accuracy.                  |
|                                 |                  | Set to '-1' to evaluate just once at the end of training.    |
+---------------------------------+------------------+--------------------------------------------------------------+
| evaluate_on_number_of_examples  | 0                | How many examples to use for hold out validation set.        |
|                                 |                  | Large values may hurt performance, e.g. model accuracy.      |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_directory       | None             | If you want to use tensorboard to visualize training         |
|                                 |                  | metrics, set this option to a valid output directory. You    |
|                                 |                  | can view the training metrics after training in tensorboard  |
|                                 |                  | via 'tensorboard --logdir <path-to-given-directory>'.        |
+---------------------------------+------------------+--------------------------------------------------------------+
| tensorboard_log_level           | "epoch"          | Define when training metrics for tensorboard should be       |
|                                 |                  | logged. Either after every epoch ('epoch') or for every      |
|                                 |                  | training step ('minibatch').                                 |
+---------------------------------+------------------+--------------------------------------------------------------+
```

</details>

:::caution
If `evaluate_on_number_of_examples` is non zero, random examples will be picked by stratified split and
used as **hold out** validation set, so they will be excluded from training data.
We suggest to set it to zero if data set contains a lot of unique examples of dialogue turns.

:::

:::note
For `cosine` similarity `maximum_positive_similarity` and `maximum_negative_similarity` should
be between `-1` and `1`.

:::

:::note
There is an option to use linearly increasing batch size. The idea comes from
[https://arxiv.org/abs/1711.00489](https://arxiv.org/abs/1711.00489). In order to do it pass a list to `batch_size`, e.g.
`"batch_size": [8, 32]` (default behavior). If constant `batch_size` is required, pass an `int`,
e.g. `"batch_size": 8`.

:::

:::note
The parameter `maximum_negative_similarity` is set to a negative value to mimic the original
starspace algorithm in the case `maximum_negative_similarity = maximum_positive_similarity` and
`use_maximum_negative_similarity = False`. See [starspace paper](https://arxiv.org/abs/1709.03856)
for details.

:::

## Rule Policy

The `Rule Policy` is a policy that handles conversation parts that follow
a fixed behavior. Please see [Rules](./rules.mdx) for further information.

## Memoization Policy

The `MemoizationPolicy` remembers the stories from your
training data. It checks if the current conversation matches a story
in the training data. If so, it will predict the next action from the matching
story of your training data with a confidence of `1.0`. If no matching conversation
is found, the policy predicts `None` with confidence `0.0`.

When looking for a match in your training data, the policy will take the last
`max_history` number of turns of the conversation into account.
One “turn” includes the message sent by the user and any actions the
assistant performed before waiting for the next message.

You can configure the number of turns the `MemoizationPolicy` should use in your
configuration:
```yaml title="config.yml"
policies:
  - name: "MemoizationPolicy"
    max_history: 3
```


## Augmented Memoization Policy

The `AugmentedMemoizationPolicy` remembers examples from training
stories for up to `max_history` turns, just like the `MemoizationPolicy`.
Additionally, it has a forgetting mechanism that will forget a certain amount
of steps in the conversation history and try to find a match in your stories
with the reduced history. It predicts the next action with confidence `1.0`
if a match is found, otherwise it predicts `None` with confidence `0.0`.

:::note
If you have dialogues where some slots that are set during
prediction time might not be set in training stories (e.g. in training
stories starting with a reminder not all previous slots are set),
make sure to add the relevant stories without slots to your training
data as well.

:::

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="mapping-policy"></a>

## Mapping Policy

:::caution
The `MappingPolicy` is deprecated. Please see [Rules](./rules.mdx) how to implement
its behavior using the [Rule Policy](./policies.mdx#rule-policy). If you previously used
the `MappingPolicy`, see the
[migration guide](./migration-guide.mdx#migrating-from-the-mapping-policy).
:::

The `MappingPolicy` can be used to directly map intents to actions. The
mappings are assigned by giving an intent the property `triggers`, e.g.:

```yaml-rasa
intents:
 - ask_is_bot:
     triggers: action_is_bot
```

An intent can only be mapped to at most one action. The bot will run
the mapped action once it receives a message of the triggering intent. Afterwards,
it will listen for the next message. With the next
user message, normal prediction will resume.

If you do not want your intent-action mapping to affect the dialogue
history, the mapped action must return a `UserUtteranceReverted()`
event. This will delete the user's latest message, along with any events that
happened after it, from the dialogue history. This means you should not
include the intent-action interaction in your stories.

For example, if a user asks “Are you a bot?” off-topic in the middle of the
flow, you probably want to answer without that interaction affecting the next
action prediction. A triggered custom action can do anything, but here's a
simple example that dispatches a bot utterance and then reverts the interaction:

```python
class ActionIsBot(Action):
  """Revertible mapped action for utter_is_bot"""

  def name(self):
      return "action_is_bot"

  def run(self, dispatcher, tracker, domain):
      dispatcher.utter_template(template="utter_is_bot")
      return [UserUtteranceReverted()]
```

:::note
If you use the `MappingPolicy` to predict bot utterance actions directly (e.g.
`triggers: utter_{}`), these interactions must go in your stories, as in this
case there is no `UserUtteranceReverted()` and the
intent and the mapped response action will appear in the dialogue history.

:::

:::note
The MappingPolicy is also responsible for executing the default actions `action_back`
and `action_restart` in response to `/back` and `/restart`. If it is not included
in your policy example these intents will not work.

:::

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="fallback-policy"></a>

## Fallback Policy

:::caution
The `FallbackPolicy` is deprecated. Please see [Fallback Actions](./fallback-handoff.mdx#fallbackactions) how to implement
its behavior using the [Rule Policy](./policies.mdx#rule-policy). If you previously used
the `FallbackPolicy`, see the
[migration guide](./migration-guide.mdx#migrating-from-the-fallback-policy).
:::

The `FallbackPolicy` invokes a predefined action if at least one of the following occurs:

1. The intent recognition has a confidence below `nlu_threshold`.

2. The highest ranked intent differs in confidence with the second highest
   ranked intent by less than `ambiguity_threshold`.

3. None of the dialogue policies predict an action with confidence higher than `core_threshold`.

**Configuration:**

The thresholds and fallback action can be adjusted in the policy configuration
file as parameters of the `FallbackPolicy`:

```yaml-rasa
policies:
  - name: "FallbackPolicy"
    nlu_threshold: 0.3
    ambiguity_threshold: 0.1
    core_threshold: 0.3
    fallback_action_name: 'action_default_fallback'
```

|                       |                                                                                                                 |
|-----------------------|-----------------------------------------------------------------------------------------------------------------|
|`nlu_threshold`        |Min confidence needed to accept an NLU prediction                                                                |
|`ambiguity_threshold`  |Min amount by which the confidence of the top intent must exceed that of the second highest ranked intent.       |
|`core_threshold`       |Min confidence needed to accept an action prediction from Rasa Core                                              |
|`fallback_action_name` |Name of the fallback action to be called if the confidence of intent or action is below the respective threshold |

`action_default_fallback` is a default action in Rasa Core which sends the
`utter_default` response to the user (you will have to define this response
in the `responses:` section of your domain file). You can also choose your
`fallback_action_name` to be one of your custom actions.

You can also configure the `FallbackPolicy` in your python code:

```python
from rasa.core.policies.fallback import FallbackPolicy
from rasa.core.policies.keras_policy import TEDPolicy
from rasa.core.agent import Agent

fallback = FallbackPolicy(fallback_action_name="action_default_fallback",
                          core_threshold=0.3,
                          nlu_threshold=0.3,
                          ambiguity_threshold=0.1)

agent = Agent("domain.yml", policies=[TEDPolicy(), fallback])
```

:::note
You can include either the `FallbackPolicy` or the
`TwoStageFallbackPolicy` in your configuration, but not both.

:::

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="two-stage-fallback-policy"></a>

## Two-Stage Fallback Policy

:::caution
The `TwoStageFallbackPolicy` is deprecated. Please see [Fallback Actions](./fallback-handoff.mdx#fallbackactions) how to implement
its behavior using the [Rule Policy](./policies.mdx#rule-policy). If you previously used
the `TwoStageFallbackPolicy`, see the
[migration guide](./migration-guide.mdx#migrating-from-the-two-stage-fallback-policy).
:::

The `TwoStageFallbackPolicy` handles low NLU confidence in multiple stages
by trying to disambiguate the user input.

* If an NLU prediction has a low confidence score or is not significantly higher
  than the second highest ranked prediction, the user is asked to affirm
  the classification of the intent.

    * If they affirm, the story continues as if the intent was classified
      with high confidence from the beginning.

    * If they deny, the user is asked to rephrase their message.

* Rephrasing

    * If the classification of the rephrased intent was confident, the story
      continues as if the user had this intent from the beginning.

    * If the rephrased intent was not classified with high confidence, the user
      is asked to affirm the classified intent.

* Second affirmation

    * If the user affirms the intent, the story continues as if the user had
      this intent from the beginning.

    * If the user denies, the original intent is classified as the specified
      `deny_suggestion_intent_name`, and an ultimate fallback action
      is triggered (e.g. a handoff to a human).

### Configuration

Rasa provides the default implementations of
`action_default_ask_affirmation` and `action_default_ask_rephrase`.
The default implementation of `action_default_ask_rephrase` action utters
the response `utter_ask_rephrase`, so be sure to specify this
response in your domain file.
The implementation of both actions can be overwritten with [custom actions](./actions#custom-actions).

You can specify the core fallback action as well as the ultimate NLU
fallback action as parameters to `TwoStageFallbackPolicy` in your
policy configuration file.

```yaml-rasa
policies:
  - name: TwoStageFallbackPolicy
    nlu_threshold: 0.3
    core_threshold: 0.3
    ambiguity_threshold: 0.1
    fallback_core_action_name: "action_default_fallback"
    fallback_nlu_action_name: "action_default_fallback"
    deny_suggestion_intent_name: "out_of_scope"
```

|                              |                                                                                                                                                                                      |
|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|`nlu_threshold`               |Min confidence needed to accept an NLU prediction                                                                                                                                     |
|`ambiguity_threshold`         |Min amount by which the confidence of the top intent must exceed that of the second highest ranked intent.                                                                            |
|`core_threshold`              |Min confidence needed to accept an action prediction from Rasa Core                                                                                                                   |
|`fallback_core_action_name`   |Name of the fallback action to be called if the confidence of Rasa Core action prediction is below the `core_threshold`. This action is to propose the recognized intents             |
|`fallback_nlu_action_name`    |Name of the fallback action to be called if the confidence of Rasa NLU intent classification is below the `nlu_threshold`. This action is called when the user denies the second time |
|`deny_suggestion_intent_name` |The name of the intent which is used to detect that the user denies the suggested intents                                                                                             |

:::note
You can include either the `FallbackPolicy` or the
`TwoStageFallbackPolicy` in your configuration, but not both.

:::

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="form-policy"></a>

## Form Policy

:::caution
The `FormPolicy` is deprecated. Please see [Forms](./forms.mdx) how to implement
its behavior using the [Rule Policy](./policies.mdx#rule-policy). If you previously used
the `FormPolicy`, see the
[migration guide](./migration-guide.mdx#form-policy).
:::

The `FormPolicy` is an extension of the `MemoizationPolicy` which
handles the filling of forms. Once a `FormAction` is called, the
`FormPolicy` will continually predict the `FormAction` until all required
slots in the form are filled.


## Conversation Featurization

<a aria-hidden="true" tabIndex="-1" className="anchor enhancedAnchor" id="featurization-conversations"></a>

In order to apply machine learning algorithms to conversational AI, we need
to build up vector representations of conversations.

Each story corresponds to a tracker which consists of the states of the
conversation just before each action was taken.

### State Featurizers

Every event in a trackers history creates a new state (e.g. running a bot
action, receiving a user message, setting slots). Featurizing a single state
of the tracker has a couple steps:

1. **Tracker provides a bag of active features**:

       * features indicating intents and entities, if this is the first
         state in a turn, e.g. it's the first action we will take after
         parsing the user's message. (e.g.
         `[intent_restaurant_search, entity_cuisine]` )

       * features indicating which slots are currently defined, e.g.
         `slot_location` if the user previously mentioned the area
         they're searching for restaurants.

       * features indicating the results of any API calls stored in
         slots, e.g. `slot_matches`

       * features indicating what the last action was (e.g.
         `prev_action_listen`)

2. **Convert all the features into numeric vectors**:

   We use the `X, y` notation that's common for supervised learning,
   where `X` is an array of shape
   `(num_data_points, time_dimension, num_input_features)`,
   and `y` is an array of shape `(num_data_points, num_bot_features)`
   or `(num_data_points, time_dimension, num_bot_features)`
   containing the target class labels encoded as one-hot vectors.

   The target labels correspond to actions taken by the bot.
   To convert the features into vector format, there are different
   featurizers available:

     * `BinarySingleStateFeaturizer` creates a binary one-hot encoding:

         The vectors `X, y` indicate a presence of a certain intent,
         entity, previous action or slot e.g. `[0 0 1 0 0 1 ...]`.

     * `LabelTokenizerSingleStateFeaturizer` creates a vector

         based on the feature label:
         All active feature labels (e.g. `prev_action_listen`) are split
         into tokens and represented as a bag-of-words. For example, actions
         `utter_explain_details_hotel` and
         `utter_explain_details_restaurant` will have 3 features in
         common, and differ by a single feature indicating a domain.

         Labels for user inputs (intents, entities) and bot actions
         are featurized separately. Each label in the two categories
         is tokenized on a special character `split_symbol`
         (e.g. `action_search_restaurant = {action, search, restaurant}`),
         creating two vocabularies. A bag-of-words representation
         is then created for each label using the appropriate vocabulary.
         The slots are featurized as binary vectors, indicating
         their presence or absence at each step of the dialogue.

:::note
If the domain defines the possible `actions`,
`[ActionGreet, ActionGoodbye]`,
`4` additional default actions are added:
`[ActionListen(), ActionRestart(),
ActionDefaultFallback(), ActionDeactivateForm()]`.
Therefore, label `0` indicates default action listen, label `1`
default restart, label `2` a greeting and `3` indicates goodbye.

:::

### Tracker Featurizers

It's often useful to include a bit more history than just the current state
when predicting an action. The `TrackerFeaturizer` iterates over tracker
states and calls a `SingleStateFeaturizer` for each state. There are two
different tracker featurizers:

#### 1. Full Dialogue

`FullDialogueTrackerFeaturizer` creates numerical representation of
stories to feed to a recurrent neural network where the whole dialogue
is fed to a network and the gradient is backpropagated from all time steps.
Therefore, `X` is an array of shape
`(num_stories, max_dialogue_length, num_input_features)` and
`y` is an array of shape
`(num_stories, max_dialogue_length, num_bot_features)`.
The smaller dialogues are padded with `-1` for all features, indicating
no values for a policy.

#### 2. Max History

`MaxHistoryTrackerFeaturizer` creates an array of previous tracker
states for each bot action or utterance, with the parameter
`max_history` defining how many states go into each row in `X`.
Deduplication is performed to filter out duplicated turns (bot actions
or bot utterances) in terms of their previous states. Hence `X`
has shape `(num_unique_turns, max_history, num_input_features)`
and `y` is an array of shape `(num_unique_turns, num_bot_features)`.

For some algorithms a flat feature vector is needed, so `X`
should be reshaped to
`(num_unique_turns, max_history \* num_input_features)`. If numeric
target class labels are needed instead of one-hot vectors, use
`y.argmax(axis=-1)`.
